---
layout: post
title: 非线性激活
author: LemonWhale
tags:
  - Pytorch
  - 深度学习
---
### ReLu函数
$$
\operatorname{ReLU}(x)=(x)^{+}=\max (0, x)
$$
### Sigmoid函数
$$
\operatorname{Sigmoid}(x)=\sigma(x)=\frac{1}{1+\exp (-x)}
$$
![sigmoid输入](/attachment/深度学习/sigmoid输入.png)

![sigmoid输入](/attachment/深度学习/sigmoid输出.png)